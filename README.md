# Explainable RAG for Smart Contract Security

## Overview
This research project explores **Retrieval-Augmented Generation (RAG)** for explainable AI in **smart contract vulnerability detection**. The goal is to improve **explainability** in security assessments, enabling **auditors and developers** to better trust AI-driven insights.

## Project Structure
```
.
├── LICENSE                     # License information
├── README.md                   # Project documentation (this file)
├── dataset/                    # Manually verified Solidity contracts dataset
│   ├── manually-verified-test/  # Test set (AST, CFG, raw source)
│   └── manually-verified-train/ # Training set (AST, CFG, raw source)
├── explanations/               # AI-generated explanations
│   ├── baseline/                # Explanations from standard LLMs
│   ├── k_analysis/              # Effect of retrieved contract count (k)
│   ├── models_comparison/       # LLM comparison (GPT-3.5, GPT-4o, etc.)
│   └── multirun_analysis/       # Multiple runs of the same model
├── knowledge_base.db           # Indexed knowledge base for retrieval
├── log/                        # Logs and runtime outputs
├── notebooks/                  # Jupyter notebooks for analysis
│   └── rag-results.ipynb        # Main results visualization notebook
├── requirements.txt            # Python dependencies
├── scripts/                    # Preprocessing and retrieval scripts
│   ├── compile_sc.sh            # Solidity compilation
│   ├── preprocess_ast.py        # AST processing
│   ├── source2ast.sh            # Convert Solidity to AST
│   └── source2cfg.py            # Convert Solidity to CFG
└── src/                        # Source code
    ├── classes/                 # Utility and RAG classes
    ├── config/                  # Configuration files (e.g., OpenAI API keys)
    ├── functions/               # Core functions (retrieval, explainability)
    └── scripts/                 # Main execution scripts
        ├── explainability.py    # Explanation generation pipeline
        ├── xrag.py              # Main RAG-based retrieval pipeline
        └── xrag.sh              # Shell script to run RAG pipeline
```

## Key Features
- **Knowledge Base Construction**: Indexed dataset of **smart contracts** categorized as vulnerable or safe.
- **Retrieval Mechanism**: Uses **AST/CFG-based graph similarity search** to retrieve relevant contract examples.
- **AI-Driven Classification & Explanation**: **LLMs** process retrieved contracts to generate **context-aware** security insights.
- **Expert Validation**: Security auditors **review AI-generated explanations** to assess accuracy and usability.

## Experimental Setup
- **Scope**: Detecting **reentrancy vulnerabilities** in Solidity contracts.
- **Dataset**: Manually verified contracts (588 data points, balanced).
- **Models Evaluated**:
  - **Baseline ML**: BERT, LSTM, FFNN, GB, XGB, KNN, LR, RF, SVM
  - **LLMs**: GPT-3.5-Turbo, GPT-4o, GPT-4o-mini, O3-mini
  - **RAG Variants**: AST, CFG, AST+CFG retrieval strategies

## Results Summary
- **Model Performance**: LLMs outperform traditional ML models across **accuracy, precision, recall, and F1**.
- **Effect of Retrieval Data Type**: AST retrieval performs **similarly** to AST+CFG, while CFG alone peaks at a higher k-value.
- **Effect of Number of Retrieved Contracts**: Best results occur at **K=3 for AST & Aggregated strategies**, while **CFG peaks at K=5**.
- **Explainability Insights**:
  - LLM-only explanations tend to be **poor and unreliable**.
  - RAG explanations are **more accurate** but require **further refinement**.
  - Expert feedback integration is key to improving **trust in AI-driven assessments**.

## Future Research Directions
- **Expanding the Knowledge Base** with **more diverse, real-world contracts** and **synthetic datasets generated by LLMs**.
- **Exploring More Advanced LLMs** with **reasoning capabilities** (e.g., Deep Seek) and testing **local AI models**.
- **Refining Retrieval Strategies** using **graph-based refinements and context-aware embeddings**.
- **Automating Expert Validation** through **reinforcement learning and AI self-improvement cycles**.

## Installation & Usage
### Prerequisites
- Python 3.8+
- Install dependencies:
  ```bash
  pip install -r requirements.txt
  ```
- Set up OpenAI API key in `config/openai.env` (if using GPT models).

### Running the Pipeline
1. **Preprocess dataset** (AST/CFG extraction):
   ```bash
   bash scripts/source2ast.sh
   python scripts/preprocess_ast.py
   python scripts/source2cfg.py
   ```
2. **Run Retrieval-Augmented Generation (RAG) pipeline**:
   ```bash
   python src/scripts/xrag.py
   ```
3. **Generate baseline explanations**:
   ```bash
   python src/scripts/explainability.py
   ```
4. **Analyze results in Jupyter Notebook**:
   ```bash
   jupyter notebook notebooks/rag-results.ipynb
   ```

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
idge the gap between AI-driven vulnerability detection and explainability, ensuring that security assessments are not only accurate but also interpretable for practitioners.**